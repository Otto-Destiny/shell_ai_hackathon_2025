{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15530bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 1.13.0\n",
      "ERROR: Ignored the following versions that require a different python version: 0.10.0 Requires-Python >=3.6,<3.9; 0.10.0.dev0 Requires-Python >=3.6,<3.9; 0.10.1 Requires-Python >=3.6,<3.9; 0.10.1.dev0 Requires-Python >=3.6,<3.9; 0.11.0 Requires-Python >=3.6,<3.9; 0.11.0.dev0 Requires-Python >=3.6,<3.9; 0.12.0 Requires-Python >=3.6,<3.9; 0.12.0.dev0 Requires-Python >=3.6,<3.9; 0.12.0.dev1 Requires-Python >=3.6,<3.9; 0.12.1 Requires-Python >=3.6,<3.9; 0.12.1.dev0 Requires-Python >=3.6,<3.9; 0.13.0 Requires-Python >=3.6,<3.10; 0.13.0.dev0 Requires-Python >=3.6,<3.10; 0.13.1 Requires-Python >=3.6,<3.10; 0.13.1.dev0 Requires-Python >=3.6,<3.10; 0.14.0 Requires-Python >=3.6,<3.10; 0.14.0.dev0 Requires-Python >=3.6,<3.10; 0.14.0.dev1 Requires-Python >=3.6,<3.10; 0.14.0.dev2 Requires-Python >=3.6,<3.10; 0.14.1 Requires-Python >=3.6,<3.10; 0.14.1.dev0 Requires-Python >=3.6,<3.10; 0.15.0 Requires-Python >=3.6,<3.10; 0.15.0.dev0 Requires-Python >=3.6,<3.10; 0.15.0.dev1 Requires-Python >=3.6,<3.10; 0.16.0 Requires-Python >=3.6,<3.10; 0.16.0.dev0 Requires-Python >=3.6,<3.10; 0.16.0.dev1 Requires-Python >=3.6,<3.10; 0.16.0.dev2 Requires-Python >=3.6,<3.10; 0.16.0.dev3 Requires-Python >=3.6,<3.10; 0.16.0.dev4 Requires-Python >=3.6,<3.10; 0.16.0.dev5 Requires-Python >=3.6,<3.10; 0.17.0 Requires-Python >=3.6,<3.10; 0.17.0.dev0 Requires-Python >=3.6,<3.10; 0.17.0.dev1 Requires-Python >=3.6,<3.10; 0.17.0.dev2 Requires-Python >=3.6,<3.10; 0.17.1 Requires-Python >=3.6,<3.10; 0.17.1.dev0 Requires-Python >=3.6,<3.10; 0.17.2 Requires-Python >=3.6,<3.10; 0.17.2.dev0 Requires-Python >=3.6,<3.10; 0.18.0 Requires-Python >=3.7,<3.11; 0.18.0.dev0 Requires-Python >=3.7,<3.11; 0.3.3 Requires-Python >=3.5,<3.8; 0.3.4 Requires-Python >=3.5,<3.8; 0.3.4.dev0 Requires-Python >=3.5,<3.8; 0.3.5 Requires-Python >=3.5,<3.8; 0.3.6 Requires-Python >=3.5,<3.8; 0.3.6.dev0 Requires-Python >=3.5,<3.8; 0.4.0 Requires-Python >=3.5,<3.9; 0.4.0.dev0 Requires-Python >=3.5,<3.9; 0.4.1 Requires-Python >=3.5,<3.9; 0.4.1.dev0 Requires-Python >=3.5,<3.9; 0.4.2 Requires-Python >=3.5,<3.9; 0.4.3 Requires-Python >=3.5,<3.9; 0.4.4 Requires-Python >=3.5,<3.9; 0.4.4.dev0 Requires-Python >=3.5,<3.9; 0.4.5 Requires-Python >=3.6,<3.9; 0.4.6.dev0 Requires-Python >=3.6,<3.9; 0.4.6.dev1 Requires-Python >=3.6,<3.9; 0.4.6.dev2 Requires-Python >=3.6,<3.9; 0.5.0 Requires-Python >=3.6,<3.9; 0.5.0.dev0 Requires-Python >=3.6,<3.9; 0.6.0 Requires-Python >=3.6,<3.9; 0.6.0.dev0 Requires-Python >=3.6,<3.9; 0.6.1 Requires-Python >=3.6,<3.9; 0.6.2.dev0 Requires-Python >=3.6,<3.9; 0.6.2.dev1 Requires-Python >=3.6,<3.9; 0.6.2.dev2 Requires-Python >=3.6,<3.9; 0.7.0 Requires-Python >=3.6,<3.9; 0.7.0.dev0 Requires-Python >=3.6,<3.9; 0.7.0.dev1 Requires-Python >=3.6,<3.9; 0.8.0 Requires-Python >=3.6,<3.9; 0.8.0.dev0 Requires-Python >=3.6,<3.9; 0.9.0 Requires-Python >=3.6,<3.9; 0.9.0.dev0 Requires-Python >=3.6,<3.9; 0.9.1 Requires-Python >=3.6,<3.9; 0.9.1.dev0 Requires-Python >=3.6,<3.9; 0.9.1.dev1 Requires-Python >=3.6,<3.9; 1.0.0 Requires-Python >=3.7,<3.11; 1.0.0b0 Requires-Python >=3.7,<3.11; 1.0.0b1 Requires-Python >=3.7,<3.11; 1.0.0rc0 Requires-Python >=3.7,<3.11; 1.0.1 Requires-Python >=3.7,<3.11; 1.0.1.dev0 Requires-Python >=3.7,<3.11; 1.1.0 Requires-Python >=3.7,<3.11; 1.1.0.dev0 Requires-Python >=3.7,<3.11; 1.2.0 Requires-Python >=3.7,<3.11; 1.2.0.dev0 Requires-Python >=3.7,<3.11; 1.2.0.dev1 Requires-Python >=3.7,<3.11; 1.2.1 Requires-Python >=3.8,<3.11; 1.2.1.dev0 Requires-Python >=3.8,<3.11; 1.2.2.dev0 Requires-Python >=3.8,<3.11; 1.2.2.dev1 Requires-Python >=3.8,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement sdv==0.18.3 (from versions: 0.0.0, 0.1.0, 0.1.1, 0.1.2, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.3.1, 0.3.2, 1.3.0.dev0, 1.3.0.dev1, 1.3.0, 1.4.0.dev0, 1.4.0.dev1, 1.4.0, 1.5.0.dev0, 1.5.0, 1.6.0.dev0, 1.6.0.dev1, 1.6.0, 1.7.0.dev0, 1.7.0, 1.8.0.dev0, 1.8.0, 1.9.0.dev0, 1.9.0, 1.10.0.dev0, 1.10.0, 1.11.0.dev0, 1.11.0, 1.12.0.dev0, 1.12.0, 1.12.1.dev0, 1.12.1.dev1, 1.12.1, 1.13.0.dev0, 1.13.1.dev0, 1.13.1, 1.14.0.dev0, 1.14.0, 1.15.0.dev0, 1.15.0, 1.16.0.dev0, 1.16.0, 1.16.1.dev0, 1.16.1, 1.16.2.dev0, 1.16.2, 1.17.0.dev0, 1.17.0.dev1, 1.17.0, 1.17.1.dev0, 1.17.1, 1.17.2.dev0, 1.17.2.dev1, 1.17.2, 1.17.3.dev0, 1.17.3, 1.17.4.dev0, 1.17.4, 1.18.0.dev0, 1.18.0, 1.19.0.dev0, 1.19.0, 1.20.0.dev0, 1.20.0, 1.20.1.dev0, 1.20.1, 1.21.0.dev0, 1.21.0, 1.22.0.dev0, 1.22.0, 1.22.1.dev0, 1.22.1, 1.23.0.dev0, 1.23.0, 1.24.0.dev0, 1.24.0, 1.24.1.dev0, 1.24.1, 1.24.2.dev0, 1.25.0.dev0, 1.25.0, 1.26.0.dev0)\n",
      "ERROR: No matching distribution found for sdv==0.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip -q install sdv==0.18.3 ctgan==0.7.4 pandas==2.2.2 numpy==1.26.4 scikit-learn==1.5.1 scipy==1.13.1 tqdm==4.66.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: Step 1 -> Step 3 (continuous-only)\n",
    "# Paste into Colab/Jupyter. If Colab, uncomment pip installs below.\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, entropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "# Generators\n",
    "from sdv.tabular import GaussianCopula\n",
    "from ctgan import TVAE  # TVAE from ctgan is fine for continuous data\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# ----------------------------- CONFIG ----------------------------------------\n",
    "CONFIG = {\n",
    "    # === Data ===\n",
    "    \"CSV_PATH\": \"your_data.csv\",      # <-- Change: path to your CSV with 55 numeric features\n",
    "    \"TARGET_COLUMNS\": [],             # leave empty: we treat all columns as features unless you list targets\n",
    "    \"FEATURE_COLUMNS\": None,          # None => auto-use all non-target columns\n",
    "    \"ALL_CONTINUOUS\": True,           # True since you said all 55 are continuous real values\n",
    "\n",
    "    # === Generator choice & sizes ===\n",
    "    \"GENERATORS\": [\"GAUSSIAN_COPULA\", \"TVAE\"],  # Both will run and produce separate outputs\n",
    "    \"SYNTHETIC_SIZE\": 100_000,\n",
    "    \"RANDOM_SEED\": 42,\n",
    "\n",
    "    # TVAE training hyperparams\n",
    "    \"TVAE_PARAMS\": {\n",
    "        \"epochs\": 300,           # increase if underfitting; reduce if too slow\n",
    "        \"batch_size\": 512,\n",
    "        \"compress_dims\": (256, 256),\n",
    "        \"decompress_dims\": (256, 256),\n",
    "        \"embedding_dim\": 128,\n",
    "        \"l2scale\": 1e-6,\n",
    "        \"cuda\": torch.cuda.is_available(),\n",
    "        \"verbose\": True\n",
    "    },\n",
    "\n",
    "    # GaussianCopula has minimal hyperparams (fast)\n",
    "    \"GAUSSIANCOPULA_PARAMS\": {\n",
    "        # SDV GaussianCopula uses default internals; we expose nothing here but keep a placeholder.\n",
    "    },\n",
    "\n",
    "    # === Validation ===\n",
    "    \"EVAL_SAMPLE_SIZE\": 50_000,\n",
    "    \"QC_TOPK\": 10,\n",
    "    \"QUALITY_THRESHOLDS\": {\n",
    "        \"KS_warn\": 0.2,\n",
    "        \"WD_norm_warn\": 0.1,\n",
    "        \"JS_warn\": 0.1,\n",
    "        \"Corr_Frob_warn\": 0.2,\n",
    "        \"RF_AUC_warn\": 0.65,\n",
    "        \"MMD_warn\": 0.1\n",
    "    },\n",
    "\n",
    "    # === Outputs ===\n",
    "    \"OUT_DIR\": \"synth_outputs\",   # folder to save synthetic CSVs & reports\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"OUT_DIR\"], exist_ok=True)\n",
    "\n",
    "# ----------------------------- Utilities -------------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def load_data(path):\n",
    "    assert os.path.exists(path), f\"CSV not found: {path}\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def infer_features(df, target_cols=None, feature_cols=None):\n",
    "    target_cols = target_cols or []\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.columns if c not in target_cols]\n",
    "    X = df[feature_cols].copy()\n",
    "    return X, feature_cols\n",
    "\n",
    "def compute_ranges(X, overrides=None):\n",
    "    overrides = overrides or {}\n",
    "    rng = {}\n",
    "    for col in X.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X[col]):\n",
    "            lo = float(np.nanmin(X[col].values))\n",
    "            hi = float(np.nanmax(X[col].values))\n",
    "            rng[col] = (lo, hi)\n",
    "    rng.update(overrides)\n",
    "    return rng\n",
    "\n",
    "def basic_impute_numeric(X):\n",
    "    # Numeric: replace NaN with median\n",
    "    Xc = X.copy()\n",
    "    for col in Xc.columns:\n",
    "        if pd.api.types.is_numeric_dtype(Xc[col]):\n",
    "            med = np.nanmedian(Xc[col].values)\n",
    "            Xc[col] = Xc[col].fillna(med)\n",
    "        else:\n",
    "            # Should not happen for all-continuous, but keep safe fallback\n",
    "            Xc[col] = Xc[col].fillna(\"UNK\")\n",
    "    return Xc\n",
    "\n",
    "# ----------------------- Sampling & models -----------------------------------\n",
    "\n",
    "def train_gaussian_copula(X):\n",
    "    model = GaussianCopula()\n",
    "    t0 = time.time()\n",
    "    model.fit(X)\n",
    "    print(f\"GaussianCopula fit done in {(time.time()-t0)/60:.2f} min.\")\n",
    "    return model\n",
    "\n",
    "def train_tvae(X, params):\n",
    "    model = TVAE(**{k: v for k, v in params.items() if k in TVAE.__init__.__code__.co_varnames})\n",
    "    t0 = time.time()\n",
    "    model.fit(X)\n",
    "    print(f\"TVAE fit done in {(time.time()-t0)/60:.2f} min.\")\n",
    "    return model\n",
    "\n",
    "def sample_and_clip(model, n, feature_ranges, integer_cols=None, clip=True):\n",
    "    S = model.sample(n)\n",
    "    # Ensure numeric dtype alignment and same columns as feature_ranges\n",
    "    for col in S.columns:\n",
    "        if pd.api.types.is_numeric_dtype(S[col]):\n",
    "            if clip and col in feature_ranges:\n",
    "                lo, hi = feature_ranges[col]\n",
    "                S[col] = S[col].clip(lower=lo, upper=hi)\n",
    "    # Round integer columns if any\n",
    "    if integer_cols:\n",
    "        for c in integer_cols:\n",
    "            if c in S.columns:\n",
    "                S[c] = np.round(S[c]).astype(\"Int64\").astype(float)\n",
    "                if c in feature_ranges:\n",
    "                    lo, hi = feature_ranges[c]\n",
    "                    S[c] = S[c].clip(lower=math.ceil(lo), upper=math.floor(hi))\n",
    "    return S\n",
    "\n",
    "# ----------------------- Validation metrics ----------------------------------\n",
    "\n",
    "def jensen_shannon_divergence(p, q, eps=1e-12):\n",
    "    p = np.asarray(p, dtype=float) + eps\n",
    "    q = np.asarray(q, dtype=float) + eps\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def numeric_hist_js(x, y, bins=50):\n",
    "    lo = np.nanmin([np.min(x), np.min(y)])\n",
    "    hi = np.nanmax([np.max(x), np.max(y)])\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:\n",
    "        return np.nan\n",
    "    px, _ = np.histogram(x, bins=bins, range=(lo, hi), density=True)\n",
    "    py, _ = np.histogram(y, bins=bins, range=(lo, hi), density=True)\n",
    "    return jensen_shannon_divergence(px, py)\n",
    "\n",
    "def normalized_wasserstein(x, y):\n",
    "    lo = np.nanmin([np.min(x), np.min(y)])\n",
    "    hi = np.nanmax([np.max(x), np.max(y)])\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:\n",
    "        return np.nan\n",
    "    return wasserstein_distance(x, y) / (hi - lo)\n",
    "\n",
    "def per_feature_report_numeric(real_df, synth_df, qc_topk=10):\n",
    "    rows = []\n",
    "    for col in real_df.columns:\n",
    "        xr = pd.to_numeric(real_df[col], errors='coerce').dropna()\n",
    "        xs = pd.to_numeric(synth_df[col], errors='coerce').dropna()\n",
    "        if len(xr) == 0 or len(xs) == 0:\n",
    "            ks = np.nan; wd = np.nan; js = np.nan\n",
    "        else:\n",
    "            ks = ks_2samp(xr, xs).statistic\n",
    "            wd = normalized_wasserstein(xr, xs)\n",
    "            js = numeric_hist_js(xr, xs)\n",
    "        rows.append({\"column\": col, \"KS\": ks, \"Wasserstein_norm\": wd, \"JS\": js})\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"\\nTop drift by KS:\")\n",
    "    print(df.sort_values(\"KS\", ascending=False).head(qc_topk)[[\"column\", \"KS\"]])\n",
    "    print(\"\\nTop drift by Wasserstein_norm:\")\n",
    "    print(df.sort_values(\"Wasserstein_norm\", ascending=False).head(qc_topk)[[\"column\", \"Wasserstein_norm\"]])\n",
    "    print(\"\\nTop drift by JS:\")\n",
    "    print(df.sort_values(\"JS\", ascending=False).head(qc_topk)[[\"column\", \"JS\"]])\n",
    "    return df\n",
    "\n",
    "def corr_frobenius_diff(real_df, synth_df):\n",
    "    num_cols = [c for c in real_df.columns if pd.api.types.is_numeric_dtype(real_df[c])]\n",
    "    if not num_cols:\n",
    "        return {\"pearson_frob\": np.nan, \"spearman_frob\": np.nan}\n",
    "    rr = real_df[num_cols].apply(pd.to_numeric, errors='coerce').fillna(method=\"ffill\")\n",
    "    ss = synth_df[num_cols].apply(pd.to_numeric, errors='coerce').fillna(method=\"ffill\")\n",
    "    Cr = rr.corr(method=\"pearson\").fillna(0).to_numpy()\n",
    "    Cs = ss.corr(method=\"pearson\").fillna(0).to_numpy()\n",
    "    Spr = rr.corr(method=\"spearman\").fillna(0).to_numpy()\n",
    "    Sps = ss.corr(method=\"spearman\").fillna(0).to_numpy()\n",
    "    pear = np.linalg.norm(Cr - Cs, ord=\"fro\") / Cr.size\n",
    "    spear = np.linalg.norm(Spr - Sps, ord=\"fro\") / Spr.size\n",
    "    return {\"pearson_frob\": pear, \"spearman_frob\": spear}\n",
    "\n",
    "def gaussian_mmd(X, Y, sigma=None, max_samples=5000, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    if len(X) > max_samples:\n",
    "        X = X[rng.choice(len(X), max_samples, replace=False)]\n",
    "    if len(Y) > max_samples:\n",
    "        Y = Y[rng.choice(len(Y), max_samples, replace=False)]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    XY = np.vstack([X, Y])\n",
    "    XY = scaler.fit_transform(XY)\n",
    "    Xs = XY[:len(X)]\n",
    "    Ys = XY[len(X):]\n",
    "\n",
    "    if sigma is None:\n",
    "        Z = np.vstack([Xs[:1000], Ys[:1000]]) if len(Xs) > 1000 and len(Ys) > 0 else np.vstack([Xs, Ys])\n",
    "        dists = np.sum((Z[:, None, :] - Z[None, :, :]) ** 2, axis=2)\n",
    "        med = np.median(dists[np.triu_indices_from(dists, k=1)])\n",
    "        sigma = max(np.sqrt(med / 2), 1e-6)\n",
    "\n",
    "    def k(a, b):\n",
    "        return np.exp(-np.sum((a[:, None, :] - b[None, :, :]) ** 2, axis=2) / (2 * sigma ** 2))\n",
    "\n",
    "    Kxx = k(Xs, Xs)\n",
    "    Kyy = k(Ys, Ys)\n",
    "    Kxy = k(Xs, Ys)\n",
    "\n",
    "    n = len(Xs)\n",
    "    m = len(Ys)\n",
    "    mmd2 = (Kxx.sum() - np.trace(Kxx)) / (n * (n - 1)) \\\n",
    "         + (Kyy.sum() - np.trace(Kyy)) / (m * (m - 1)) \\\n",
    "         - 2 * Kxy.mean()\n",
    "    return float(np.sqrt(max(mmd2, 0.0)))\n",
    "\n",
    "def classifier_two_sample_auc(real_df, synth_df, random_state=42):\n",
    "    X = pd.concat([real_df, synth_df], axis=0).copy()\n",
    "    y = np.array([0] * len(real_df) + [1] * len(synth_df))\n",
    "    # Numeric pipeline\n",
    "    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    transformers = []\n",
    "    if num_cols:\n",
    "        transformers.append((\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols))\n",
    "    pre = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "    X_proc = pre.fit_transform(X)\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    aucs = cross_val_score(clf, X_proc, y, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    return float(aucs.mean())\n",
    "\n",
    "# ----------------------------- Runner ----------------------------------------\n",
    "\n",
    "def run_pipeline(config):\n",
    "    set_seed(config[\"RANDOM_SEED\"])\n",
    "    df = load_data(config[\"CSV_PATH\"])\n",
    "    X, feat_cols = infer_features(df, target_cols=config[\"TARGET_COLUMNS\"], feature_cols=config[\"FEATURE_COLUMNS\"])\n",
    "    print(f\"Loaded data {df.shape}; using {len(feat_cols)} features (first 8): {feat_cols[:8]}\")\n",
    "\n",
    "    # All continuous: impute numeric missing values\n",
    "    X_clean = basic_impute_numeric(X)\n",
    "\n",
    "    # Compute per-column numeric ranges (empirical)\n",
    "    feature_ranges = compute_ranges(X_clean, overrides={})\n",
    "    print(\"Sample feature ranges (first 8):\")\n",
    "    for i, (k, v) in enumerate(list(feature_ranges.items())[:8]):\n",
    "        print(f\"  {k}: {v}\")\n",
    "        if i >= 7: break\n",
    "\n",
    "    results_summary = {}\n",
    "\n",
    "    for gen_name in config[\"GENERATORS\"]:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"RUN GENERATOR: {gen_name}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        if gen_name == \"GAUSSIAN_COPULA\":\n",
    "            model = train_gaussian_copula(X_clean)\n",
    "        elif gen_name == \"TVAE\":\n",
    "            model = train_tvae(X_clean, config[\"TVAE_PARAMS\"])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown generator: \" + str(gen_name))\n",
    "\n",
    "        # Sample synthetic rows (in one shot if memory allows; generators often stream)\n",
    "        n = int(config[\"SYNTHETIC_SIZE\"])\n",
    "        print(f\"Sampling {n:,} rows from {gen_name} ...\")\n",
    "        S = sample_and_clip(model, n, feature_ranges, integer_cols=None, clip=True)\n",
    "        # Align columns\n",
    "        S = S[feat_cols]\n",
    "        out_csv = os.path.join(config[\"OUT_DIR\"], f\"synthetic_{gen_name.lower()}_{n:,}.csv\".replace(\",\", \"\"))\n",
    "        S.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved synthetic CSV: {out_csv}\")\n",
    "\n",
    "        # Validation: use subsample for speed\n",
    "        eval_n = min(config[\"EVAL_SAMPLE_SIZE\"], len(S))\n",
    "        S_eval = S.sample(eval_n, random_state=config[\"RANDOM_SEED\"]) if len(S) > eval_n else S.copy()\n",
    "        X_eval = X_clean.sample(eval_n, random_state=config[\"RANDOM_SEED\"]) if len(X_clean) > eval_n else X_clean.copy()\n",
    "\n",
    "        # 1) Per-feature metrics\n",
    "        per_feat = per_feature_report_numeric(X_eval, S_eval, qc_topk=config[\"QC_TOPK\"])\n",
    "        per_feat_csv = os.path.join(config[\"OUT_DIR\"], f\"drift_per_feature_{gen_name.lower()}.csv\")\n",
    "        per_feat.to_csv(per_feat_csv, index=False)\n",
    "\n",
    "        # 2) Correlation differences\n",
    "        corr_stats = corr_frobenius_diff(X_eval, S_eval)\n",
    "\n",
    "        # 3) Classifier two-sample AUC\n",
    "        try:\n",
    "            auc = classifier_two_sample_auc(X_eval, S_eval, random_state=config[\"RANDOM_SEED\"])\n",
    "        except Exception as e:\n",
    "            print(\"Classifier test failed:\", e)\n",
    "            auc = float(\"nan\")\n",
    "\n",
    "        # 4) MMD on numeric subset\n",
    "        num_cols_eval = [c for c in X_eval.columns if pd.api.types.is_numeric_dtype(X_eval[c])]\n",
    "        if num_cols_eval:\n",
    "            Xn = X_eval[num_cols_eval].to_numpy(dtype=float)\n",
    "            Sn = S_eval[num_cols_eval].to_numpy(dtype=float)\n",
    "            mmd = gaussian_mmd(Xn, Sn, sigma=None, max_samples=5000, random_state=config[\"RANDOM_SEED\"])\n",
    "        else:\n",
    "            mmd = float(\"nan\")\n",
    "\n",
    "        # Summary printing\n",
    "        print(\"\\nSUMMARY for\", gen_name)\n",
    "        print(f\"  Worst KS (numeric): {float(per_feat.loc[:, 'KS'].max()):.4f}\")\n",
    "        print(f\"  Worst Wasserstein_norm: {float(per_feat.loc[:, 'Wasserstein_norm'].max()):.4f}\")\n",
    "        print(f\"  Worst JS: {float(per_feat.loc[:, 'JS'].max()):.4f}\")\n",
    "        print(f\"  Pearson corr Frobenius diff: {corr_stats['pearson_frob']:.4f}\")\n",
    "        print(f\"  Spearman corr Frobenius diff: {corr_stats['spearman_frob']:.4f}\")\n",
    "        print(f\"  Classifier two-sample AUC: {auc:.4f} (0.5 ~ indistinguishable)\")\n",
    "        print(f\"  MMD (Gaussian kernel): {mmd:.4f}\")\n",
    "\n",
    "        # Save generator summary\n",
    "        results_summary[gen_name] = {\n",
    "            \"synthetic_csv\": out_csv,\n",
    "            \"per_feature_csv\": per_feat_csv,\n",
    "            \"per_feature_stats\": per_feat.to_dict(orient=\"records\"),\n",
    "            \"corr_stats\": corr_stats,\n",
    "            \"two_sample_auc\": float(auc),\n",
    "            \"mmd\": float(mmd),\n",
    "        }\n",
    "\n",
    "        # Save a small JSON summary per generator\n",
    "        summary_path = os.path.join(config[\"OUT_DIR\"], f\"summary_{gen_name.lower()}.json\")\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            json.dump(results_summary[gen_name], f, indent=2)\n",
    "        print(f\"Saved summary JSON: {summary_path}\")\n",
    "\n",
    "    # Save combined summary\n",
    "    combined_path = os.path.join(config[\"OUT_DIR\"], \"combined_summaries.json\")\n",
    "    with open(combined_path, \"w\") as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    print(f\"\\nAll generator runs complete. Combined summary saved to: {combined_path}\")\n",
    "    return results_summary\n",
    "\n",
    "# ----------------------------- Execute ---------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # EDIT: change CONFIG[\"CSV_PATH\"] before running\n",
    "    if not os.path.exists(CONFIG[\"CSV_PATH\"]):\n",
    "        raise SystemExit(f\"Please set CONFIG['CSV_PATH'] to your dataset CSV (current: {CONFIG['CSV_PATH']})\")\n",
    "    results = run_pipeline(CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99768efb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sdv.tabular'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msdv\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabular\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianCopula\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sdv.tabular'"
     ]
    }
   ],
   "source": [
    "from sdv.tabular import GaussianCopula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d50d11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sdv:\n",
      "\n",
      "NAME\n",
      "    sdv - Top-level package for SDV.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _utils\n",
      "    cag (package)\n",
      "    constraints (package)\n",
      "    data_processing (package)\n",
      "    datasets (package)\n",
      "    errors\n",
      "    evaluation (package)\n",
      "    io (package)\n",
      "    lite (package)\n",
      "    logging (package)\n",
      "    metadata (package)\n",
      "    metrics (package)\n",
      "    multi_table (package)\n",
      "    sampling (package)\n",
      "    sequential (package)\n",
      "    single_table (package)\n",
      "    utils (package)\n",
      "    version (package)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['constraints', 'data_processing', 'datasets', 'evaluation',...\n",
      "    __email__ = 'info@sdv.dev'\n",
      "\n",
      "VERSION\n",
      "    1.25.0\n",
      "\n",
      "AUTHOR\n",
      "    DataCebo, Inc.\n",
      "\n",
      "FILE\n",
      "    c:\\users\\joy otto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\sdv\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sdv\n",
    "help(sdv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
